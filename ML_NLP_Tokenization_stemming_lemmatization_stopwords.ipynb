{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPamb6NS7CmYYgHVpjuY/a4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthikraja131/Complete-Guide-to-Building-Deploying-and-Optimizing-Generative-AI-with-Langchain-and-Huggingface/blob/main/ML_NLP_Tokenization_stemming_lemmatization_stopwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "In Natural Language Processing (NLP), **tokenization** is the process of breaking down a continuous stream of text into smaller, meaningful units called **tokens**.\n",
        "\n",
        "These tokens can be:\n",
        "\n",
        "* **Words:** The most common form, where text is split into individual words (e.g., \"Hello, world!\" -> [\"Hello\", \",\", \"world\", \"!\"]).\n",
        "* **Characters:** Splitting text into individual characters.\n",
        "* **Subwords:** Units larger than characters but smaller than words, useful for handling rare words or languages with complex word structures (e.g., \"unusual\" -> [\"un\", \"usual\"]).\n",
        "* **Sentences:** Breaking a large text into individual sentences.\n",
        "\n",
        "`Tokenization is a fundamental first step in almost any NLP task,` as it converts raw, unstructured text into a format that machines can process and analyze more easily.\n",
        "\n",
        "Alright, let's refine those definitions to be accurate and clear for NLP.\n",
        "\n",
        "---\n",
        "\n",
        "## NLP Terminology\n",
        "\n",
        "* **Corpus:** A large, structured collection of texts, often used for linguistic analysis and training NLP models. Think of it as a dataset of language.\n",
        "* **Documents:** Individual pieces of text contained within a corpus. A document could be anything from a single article, book chapter, email, or even a tweet.\n",
        "* **Vocabulary:** The complete set of all unique words or tokens found across an entire corpus or within a specific document.\n",
        "* **Words:** The fundamental individual units of language that make up documents and, collectively, form the vocabulary. These are typically the result of a tokenization process."
      ],
      "metadata": {
        "id": "MGwsiC7_gXHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK (Natural Language Toolkit)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "is a powerful and widely used open-source Python library for Natural Language Processing (NLP).\n",
        "\n",
        "It provides a comprehensive suite of tools, data, and algorithms for various NLP tasks, including:\n",
        "\n",
        "* **Tokenization:** Breaking text into words or sentences.\n",
        "* **Stemming and Lemmatization:** Reducing words to their root forms.\n",
        "* **Part-of-Speech (POS) Tagging:** Identifying the grammatical role of words (e.g., noun, verb).\n",
        "* **Named Entity Recognition (NER):** Identifying and classifying named entities like people, organizations, and locations.\n",
        "* **Text Classification:** Categorizing text into predefined classes.\n",
        "* **Parsing:** Analyzing the grammatical structure of sentences.\n",
        "* **Access to Corpora:** Provides a large collection of linguistic datasets for research and development.\n",
        "\n",
        "NLTK is particularly popular for teaching and research in computational linguistics and is a great starting point for anyone learning NLP."
      ],
      "metadata": {
        "id": "8Cf19zgrqx_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#spaCy (pronounced \"space-ee\")\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "is a free, open-source Python library designed for **advanced Natural Language Processing (NLP)**, particularly focused on **production-ready applications**.\n",
        "\n",
        "While NLTK is often favored for teaching and research due to its breadth of algorithms and corpora, spaCy excels in:\n",
        "\n",
        "* **Speed and Efficiency:** It's built for performance, written in Python and Cython, making it very fast for processing large volumes of text.\n",
        "* **Production Use:** spaCy is engineered with a focus on ease of use in real-world applications, offering streamlined workflows and robust pre-trained models.\n",
        "* **Modern Design:** It provides opinionated, \"industrial-strength\" NLP solutions, often with a clear and consistent API.\n",
        "* **Pre-trained Models:** spaCy offers highly optimized statistical models for various languages (e.g., English, German, Spanish) that can perform tasks like:\n",
        "    * **Tokenization:** Efficiently breaking text into tokens.\n",
        "    * **Part-of-Speech (POS) Tagging:** Assigning grammatical tags to words.\n",
        "    * **Dependency Parsing:** Analyzing the grammatical structure of sentences to show relationships between words.\n",
        "    * **Named Entity Recognition (NER):** Identifying and classifying named entities (people, organizations, locations, dates, etc.).\n",
        "    * **Text Classification:** Categorizing text.\n",
        "    * **Word Vectors/Embeddings:** Providing numerical representations of words that capture semantic meaning.\n",
        "* **Integration with Deep Learning:** spaCy seamlessly integrates with popular deep learning frameworks like TensorFlow and PyTorch, allowing users to leverage state-of-the-art transformer models (like BERT, GPT-2).\n",
        "* **Rule-based Matching:** A powerful system for finding specific patterns in text beyond what statistical models might catch.\n",
        "\n",
        "In essence, if you're looking to build an NLP application that needs to be fast, scalable, and deployed in a real-world setting, spaCy is often the go-to choice."
      ],
      "metadata": {
        "id": "8HQQPHSbqoD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjfKlUdsfPkR",
        "outputId": "e910cefb-7cd2-4840-de9c-8e0e05b01aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Hello!, My Name is Karthikraja, Doing Data Scientist and AI Engineer. I am excel in Building AI Applications.\n",
        "And, I'm a Civil Engineer\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5h01_92ftEvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oeEOIcmOtNb3",
        "outputId": "2faeba1a-d9d1-4fb8-d46f-23b6ca80a7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello!, My Name is Karthikraja, Doing Data Scientist and AI Engineer. I am excel in Building AI Applications.\\nAnd, I'm a Civil Engineer\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZreTZ_OdtNeI",
        "outputId": "7c4da325-bdb1-47e9-ccd3-c1afc6feaa3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!, My Name is Karthikraja, Doing Data Scientist and AI Engineer. I am excel in Building AI Applications.\n",
            "And, I'm a Civil Engineer\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Sentence ---> Paragraphs\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER6m-3Mxt2hE",
        "outputId": "3fdde4f6-8e7f-494d-efed-4b98ab1b8e6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "documents = sent_tokenize(corpus)\n",
        "#documents"
      ],
      "metadata": {
        "id": "Gxx9pv6hu1K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4BPygRXvdZ9",
        "outputId": "3b4f5ed2-e77a-4714-b384-0e944e997ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello!, My Name is Karthikraja, Doing Data Scientist and AI Engineer.',\n",
              " 'I am excel in Building AI Applications.',\n",
              " \"And, I'm a Civil Engineer\"]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB_ypF6Gv-2u",
        "outputId": "f817a32e-3245-4b76-a280-0e04c8ab09ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeEgndVcv-5G",
        "outputId": "e355af56-22cf-44ed-dfa1-e676a87e375a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!, My Name is Karthikraja, Doing Data Scientist and AI Engineer.\n",
            "I am excel in Building AI Applications.\n",
            "And, I'm a Civil Engineer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "#paragraphs --> words\n",
        "#senetence --> words\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "wA4Qo5JAwJx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDTBBJs0wWK8",
        "outputId": "d97af305-c9dc-48b0-9b96-4d184fedb5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " ',',\n",
              " 'My',\n",
              " 'Name',\n",
              " 'is',\n",
              " 'Karthikraja',\n",
              " ',',\n",
              " 'Doing',\n",
              " 'Data',\n",
              " 'Scientist',\n",
              " 'and',\n",
              " 'AI',\n",
              " 'Engineer',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'excel',\n",
              " 'in',\n",
              " 'Building',\n",
              " 'AI',\n",
              " 'Applications',\n",
              " '.',\n",
              " 'And',\n",
              " ',',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'a',\n",
              " 'Civil',\n",
              " 'Engineer']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRPnGliiwWNk",
        "outputId": "801e2dfe-489f-46d0-b1b6-424aa187fd79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', ',', 'My', 'Name', 'is', 'Karthikraja', ',', 'Doing', 'Data', 'Scientist', 'and', 'AI', 'Engineer', '.']\n",
            "['I', 'am', 'excel', 'in', 'Building', 'AI', 'Applications', '.']\n",
            "['And', ',', 'I', \"'m\", 'a', 'Civil', 'Engineer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**wordpunct_tokenize**\n",
        "\n",
        "`wordpunct_tokenize` is an NLTK tokenizer that splits text based on **whitespace** and **punctuation**. It's a simple, regular-expression-based tokenizer that separates most punctuation marks into their own tokens, even when they're attached to words (e.g., \"hello!\" becomes [\"hello\", \"!\"]). This often results in more granular tokens compared to `word_tokenize` (which uses Treebank rules and might keep some punctuation attached to words)."
      ],
      "metadata": {
        "id": "3CzRKEODzXcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "05SDH5YxxQlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(wordpunct_tokenize(sentence))        # the output seperate the \" ' \" quotes also seperately at \"I'm\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw_MI1lWx7pE",
        "outputId": "c8dd8747-dc48-49f7-9d46-2533744e5d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!,', 'My', 'Name', 'is', 'Karthikraja', ',', 'Doing', 'Data', 'Scientist', 'and', 'AI', 'Engineer', '.']\n",
            "['I', 'am', 'excel', 'in', 'Building', 'AI', 'Applications', '.']\n",
            "['And', ',', 'I', \"'\", 'm', 'a', 'Civil', 'Engineer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**TreebankWordTokenizer**\n",
        "\n",
        "The **TreebankWordTokenizer** (from NLTK) is a word tokenization tool that follows the rules established by the **Penn Treebank** corpus. It's known for its consistent handling of **contractions** (e.g., \"don't\" becomes \"do\", \"n't\") and **punctuation**, separating them into individual tokens. This makes it a popular choice for NLP tasks requiring linguistically informed word boundaries."
      ],
      "metadata": {
        "id": "TwDMU51fzIbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLuo36usyimF",
        "outputId": "287f66dc-d15c-4807-c521-8cdac70a15c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " ',',\n",
              " 'My',\n",
              " 'Name',\n",
              " 'is',\n",
              " 'Karthikraja',\n",
              " ',',\n",
              " 'Doing',\n",
              " 'Data',\n",
              " 'Scientist',\n",
              " 'and',\n",
              " 'AI',\n",
              " 'Engineer.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'excel',\n",
              " 'in',\n",
              " 'Building',\n",
              " 'AI',\n",
              " 'Applications.',\n",
              " 'And',\n",
              " ',',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'a',\n",
              " 'Civil',\n",
              " 'Engineer']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NL5bMxqrGDnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Stemming\n",
        "\n",
        "**Stemming** is a text normalization technique in NLP that reduces inflected (or sometimes derived) words to their **root or base form**, often called a \"stem.\" This process primarily involves **removing affixes (suffixes and prefixes)** to group together different forms of a word (e.g., \"running,\" \"runs,\" \"runner\" might all reduce to \"run\"). While important for **Natural Language Understanding (NLU)** and **Natural Language Processing (NLP)**, it's a heuristic process that doesn't guarantee the resulting stem is a grammatically correct word."
      ],
      "metadata": {
        "id": "qlUe3t87GDp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Classification Problem\n",
        "## Comments of product is a positive review or negative review\n",
        "## Reviews----> eating, eat,eaten [going,gone,goes]--->go\n",
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\",\"running\",\"runs\"]"
      ],
      "metadata": {
        "id": "q0WWnsv9yioV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PorterStemmer"
      ],
      "metadata": {
        "id": "VDjkORVMHbFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "DO8mQQrFHYD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "Pbbo7CdPIDaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for word in words:\n",
        "  print(word, \"--->\",stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btap9653HYGp",
        "outputId": "ef6ba3c4-dfe0-410b-c233-a820e26928d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eats ---> eat\n",
            "eaten ---> eaten\n",
            "writing ---> write\n",
            "writes ---> write\n",
            "programming ---> program\n",
            "programs ---> program\n",
            "history ---> histori\n",
            "finally ---> final\n",
            "finalized ---> final\n",
            "running ---> run\n",
            "runs ---> run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "history ---> histori  # There are some meaning of word would be collapsed So, This is the drawback for the stemming."
      ],
      "metadata": {
        "id": "T4RH6-_cIsaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anther example\n",
        "stemming.stem(\"Congratulations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NMCyULZkIMVL",
        "outputId": "40fc8c48-3d88-455a-a85d-907e15bb37de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cXC8JH3eJWla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegexpStemmer class\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example\n",
        "\n",
        "---\n",
        "## `RegexpStemmer` (NLTK)\n",
        "\n",
        "The `RegexpStemmer` class in NLTK allows you to perform **custom stemming** using **regular expressions**.\n",
        "\n",
        "Unlike rule-based stemmers (like Porter or Snowball) that have pre-defined rules, `RegexpStemmer` takes one or more regular expressions as input. Any substring of a word that matches these regular expressions will be **removed** to produce the stem.\n",
        "\n",
        "This provides **flexibility** to define very specific stemming logic, which can be useful for niche applications or languages where standard stemmers might not perform optimally. However, it requires a good understanding of regular expressions and the linguistic patterns you wish to target. You also typically specify a `min` length for the resulting stem to prevent over-stemming."
      ],
      "metadata": {
        "id": "LsYh95CUJWo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemming = RegexpStemmer('ing$|s$|able$|e4')"
      ],
      "metadata": {
        "id": "BS-abCg0JEHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemming.stem(\"eating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hi-7Jz8HKvle",
        "outputId": "d51bb5ed-e687-4ab9-9551-39383f79a0c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemming.stem(\"materials\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "flGZy17oMcc5",
        "outputId": "6e88e06b-5296-4c4a-e199-94ddf4072e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'material'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Z0wmrnmZMtV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Snowball Stemmer\n",
        "\n",
        "The **Snowball Stemmer** is an improved and often more accurate version of the original Porter Stemmer. It's also known as **Porter2 Stemmer**.\n",
        "\n",
        "Key characteristics include:\n",
        "\n",
        "* **Multi-language support:** Unlike the basic Porter Stemmer, Snowball offers stemming algorithms for a variety of languages beyond English (e.g., German, French, Spanish, Russian, etc.).\n",
        "* **More aggressive/refined rules:** It often produces better stems than the original Porter Stemmer due to a more extensive and refined set of stemming rules.\n",
        "* **Foundation:** It's built using the \"Snowball\" string processing language, specifically designed for creating stemming algorithms.\n",
        "* **Efficiency:** It's designed to be efficient and fast, making it suitable for larger-scale NLP applications.\n",
        "\n",
        "It's a popular choice in NLTK for tasks where a more robust and multilingual stemming solution is required."
      ],
      "metadata": {
        "id": "5hgnBd4FMtYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "for word in words:\n",
        "  print(word+'---->'+snowball_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjYo-F5tMh51",
        "outputId": "b763e8ea-5d95-4684-d553-16addfb4e848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n",
            "running---->run\n",
            "runs---->run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# by using portar stemming\n",
        "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYi9bDilNz8z",
        "outputId": "9ec742b4-0dac-465e-956a-43b4ca5d2784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# by using snowball stemming\n",
        "snowball_stemmer.stem(\"fairly\"),snowball_stemmer.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQVQibF2O_zo",
        "outputId": "373812ed-659e-464f-ccf1-de6dcb5fb0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ttq07rEMPGLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dpL_HywXU3Qq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Lemmatization\n",
        "\n",
        "**Lemmatization** in NLP is a text normalization technique that reduces words to their **base or dictionary form, known as a lemma**. Unlike stemming, lemmatization uses **linguistic knowledge** (like a dictionary or morphological analysis) to ensure that the resulting lemma is a grammatically correct word and has meaning. For example, \"running,\" \"runs,\" and \"ran\" would all be lemmatized to \"run,\" while \"better\" would become \"good.\" It is more computationally intensive but provides more accurate and meaningful root forms than stemming."
      ],
      "metadata": {
        "id": "Io6XYOVsU3Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example"
      ],
      "metadata": {
        "id": "Z5ut6sixViTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**uses:**\n",
        "\n",
        "chatbots, machine translation, and deep text analysis, where preserving the precise dictionary form and semantic meaning of words is paramount for accurate understanding and generation."
      ],
      "metadata": {
        "id": "nDk6JT4oZo49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "zMIDjve8VnQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqdkTkdWXeGi",
        "outputId": "ae09afdd-685f-4696-efbc-5e41bff6bbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "wordnet_lemmatizer.lemmatize(\"going\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LX60-9DDVnTR",
        "outputId": "1ec9d087-4ecd-4c3c-907d-09fcffe3a81a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "wordnet_lemmatizer.lemmatize(\"going\", 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wvSxvCYXVnVy",
        "outputId": "93afe11a-b25b-445e-9e25-ec12b5fe8ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize(\"going\",pos= 'n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dlCuEtcNVng5",
        "outputId": "23a75b20-9bd3-4471-8fa9-16c3f63ca4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\",\"running\",\"runs\"]\n",
        "\n",
        "for word in words:\n",
        "  print(word+'---->'+wordnet_lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q3jEROqXr0v",
        "outputId": "0bda124c-55fb-4db1-8d92-a23c87280738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eating\n",
            "eats---->eats\n",
            "eaten---->eaten\n",
            "writing---->writing\n",
            "writes---->writes\n",
            "programming---->programming\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalized\n",
            "running---->running\n",
            "runs---->run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+'---->'+wordnet_lemmatizer.lemmatize(word, pos = 'n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkwgtfBfYF7A",
        "outputId": "c1689044-e166-4b44-bcf7-e4832cd65dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eating\n",
            "eats---->eats\n",
            "eaten---->eaten\n",
            "writing---->writing\n",
            "writes---->writes\n",
            "programming---->programming\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalized\n",
            "running---->running\n",
            "runs---->run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+'---->'+wordnet_lemmatizer.lemmatize(word, pos = 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6TBatFuYQ4F",
        "outputId": "196feaa2-10be-440c-bc7c-33a816370c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n",
            "running---->run\n",
            "runs---->run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+'---->'+wordnet_lemmatizer.lemmatize(word, pos = 'r'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b1bXUcVYTbM",
        "outputId": "f079acbd-d255-44c5-80a4-43c9500c1012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eating\n",
            "eats---->eats\n",
            "eaten---->eaten\n",
            "writing---->writing\n",
            "writes---->writes\n",
            "programming---->programming\n",
            "programs---->programs\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalized\n",
            "running---->running\n",
            "runs---->runs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize(\"fairly\",pos='v'),wordnet_lemmatizer.lemmatize(\"fairly\",pos= 'a'),wordnet_lemmatizer.lemmatize(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k8AKE2FYYBw",
        "outputId": "33458b0c-8557-4d66-b4ba-e2a18e84fb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cC6J2uktYxdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Stopwords\n",
        "\n",
        "**Stopwords** are very common words (like \"the\", \"a\", \"is\", \"and\", \"in\", \"of\") that appear frequently in almost any text but often carry **little to no unique meaning or analytical value** on their own, especially in tasks like text classification, information retrieval, or topic modeling.\n",
        "\n",
        "### Why are they used (or removed)?\n",
        "\n",
        "The primary reasons for identifying and often removing stopwords in NLP are:\n",
        "\n",
        "1.  **Reduce Noise:** They clutter the data with highly frequent but non-informative terms, making it harder to identify the truly significant words.\n",
        "2.  **Improve Efficiency:** By removing them, you reduce the vocabulary size and the dimensionality of your data, which speeds up processing and reduces computational costs for many NLP algorithms.\n",
        "3.  **Enhance Performance/Accuracy:** For tasks focused on content words, removing stopwords helps algorithms focus on the more discriminative terms, potentially improving the accuracy of models in text classification, search relevance, or sentiment analysis.\n",
        "4.  **Feature Reduction:** In machine learning models, each unique word often becomes a feature. Removing stopwords significantly reduces the number of features, leading to leaner and sometimes more robust models.\n",
        "\n",
        "It's important to note that what constitutes a \"stopword\" can be context-dependent. While general lists exist (e.g., in NLTK), sometimes domain-specific stopwords are added or general ones are kept if they become important for a particular task (e.g., \"not\" for sentiment analysis)."
      ],
      "metadata": {
        "id": "apMJwQX0cSMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwURHT9PcqiH",
        "outputId": "a86c096e-a468-4a06-f2f4-1940a22ea0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "jkS3O2TCcqkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpHGT-R4dJjD",
        "outputId": "9672c4e1-d235-4620-ab3c-8b4a809aea2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('german')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmcCkuR0dcxj",
        "outputId": "a9447c72-f32c-4e71-e2ef-b8e8bf67003c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aber',\n",
              " 'alle',\n",
              " 'allem',\n",
              " 'allen',\n",
              " 'aller',\n",
              " 'alles',\n",
              " 'als',\n",
              " 'also',\n",
              " 'am',\n",
              " 'an',\n",
              " 'ander',\n",
              " 'andere',\n",
              " 'anderem',\n",
              " 'anderen',\n",
              " 'anderer',\n",
              " 'anderes',\n",
              " 'anderm',\n",
              " 'andern',\n",
              " 'anderr',\n",
              " 'anders',\n",
              " 'auch',\n",
              " 'auf',\n",
              " 'aus',\n",
              " 'bei',\n",
              " 'bin',\n",
              " 'bis',\n",
              " 'bist',\n",
              " 'da',\n",
              " 'damit',\n",
              " 'dann',\n",
              " 'der',\n",
              " 'den',\n",
              " 'des',\n",
              " 'dem',\n",
              " 'die',\n",
              " 'das',\n",
              " 'dass',\n",
              " 'da',\n",
              " 'derselbe',\n",
              " 'derselben',\n",
              " 'denselben',\n",
              " 'desselben',\n",
              " 'demselben',\n",
              " 'dieselbe',\n",
              " 'dieselben',\n",
              " 'dasselbe',\n",
              " 'dazu',\n",
              " 'dein',\n",
              " 'deine',\n",
              " 'deinem',\n",
              " 'deinen',\n",
              " 'deiner',\n",
              " 'deines',\n",
              " 'denn',\n",
              " 'derer',\n",
              " 'dessen',\n",
              " 'dich',\n",
              " 'dir',\n",
              " 'du',\n",
              " 'dies',\n",
              " 'diese',\n",
              " 'diesem',\n",
              " 'diesen',\n",
              " 'dieser',\n",
              " 'dieses',\n",
              " 'doch',\n",
              " 'dort',\n",
              " 'durch',\n",
              " 'ein',\n",
              " 'eine',\n",
              " 'einem',\n",
              " 'einen',\n",
              " 'einer',\n",
              " 'eines',\n",
              " 'einig',\n",
              " 'einige',\n",
              " 'einigem',\n",
              " 'einigen',\n",
              " 'einiger',\n",
              " 'einiges',\n",
              " 'einmal',\n",
              " 'er',\n",
              " 'ihn',\n",
              " 'ihm',\n",
              " 'es',\n",
              " 'etwas',\n",
              " 'euer',\n",
              " 'eure',\n",
              " 'eurem',\n",
              " 'euren',\n",
              " 'eurer',\n",
              " 'eures',\n",
              " 'fr',\n",
              " 'gegen',\n",
              " 'gewesen',\n",
              " 'hab',\n",
              " 'habe',\n",
              " 'haben',\n",
              " 'hat',\n",
              " 'hatte',\n",
              " 'hatten',\n",
              " 'hier',\n",
              " 'hin',\n",
              " 'hinter',\n",
              " 'ich',\n",
              " 'mich',\n",
              " 'mir',\n",
              " 'ihr',\n",
              " 'ihre',\n",
              " 'ihrem',\n",
              " 'ihren',\n",
              " 'ihrer',\n",
              " 'ihres',\n",
              " 'euch',\n",
              " 'im',\n",
              " 'in',\n",
              " 'indem',\n",
              " 'ins',\n",
              " 'ist',\n",
              " 'jede',\n",
              " 'jedem',\n",
              " 'jeden',\n",
              " 'jeder',\n",
              " 'jedes',\n",
              " 'jene',\n",
              " 'jenem',\n",
              " 'jenen',\n",
              " 'jener',\n",
              " 'jenes',\n",
              " 'jetzt',\n",
              " 'kann',\n",
              " 'kein',\n",
              " 'keine',\n",
              " 'keinem',\n",
              " 'keinen',\n",
              " 'keiner',\n",
              " 'keines',\n",
              " 'knnen',\n",
              " 'knnte',\n",
              " 'machen',\n",
              " 'man',\n",
              " 'manche',\n",
              " 'manchem',\n",
              " 'manchen',\n",
              " 'mancher',\n",
              " 'manches',\n",
              " 'mein',\n",
              " 'meine',\n",
              " 'meinem',\n",
              " 'meinen',\n",
              " 'meiner',\n",
              " 'meines',\n",
              " 'mit',\n",
              " 'muss',\n",
              " 'musste',\n",
              " 'nach',\n",
              " 'nicht',\n",
              " 'nichts',\n",
              " 'noch',\n",
              " 'nun',\n",
              " 'nur',\n",
              " 'ob',\n",
              " 'oder',\n",
              " 'ohne',\n",
              " 'sehr',\n",
              " 'sein',\n",
              " 'seine',\n",
              " 'seinem',\n",
              " 'seinen',\n",
              " 'seiner',\n",
              " 'seines',\n",
              " 'selbst',\n",
              " 'sich',\n",
              " 'sie',\n",
              " 'ihnen',\n",
              " 'sind',\n",
              " 'so',\n",
              " 'solche',\n",
              " 'solchem',\n",
              " 'solchen',\n",
              " 'solcher',\n",
              " 'solches',\n",
              " 'soll',\n",
              " 'sollte',\n",
              " 'sondern',\n",
              " 'sonst',\n",
              " 'ber',\n",
              " 'um',\n",
              " 'und',\n",
              " 'uns',\n",
              " 'unsere',\n",
              " 'unserem',\n",
              " 'unseren',\n",
              " 'unser',\n",
              " 'unseres',\n",
              " 'unter',\n",
              " 'viel',\n",
              " 'vom',\n",
              " 'von',\n",
              " 'vor',\n",
              " 'whrend',\n",
              " 'war',\n",
              " 'waren',\n",
              " 'warst',\n",
              " 'was',\n",
              " 'weg',\n",
              " 'weil',\n",
              " 'weiter',\n",
              " 'welche',\n",
              " 'welchem',\n",
              " 'welchen',\n",
              " 'welcher',\n",
              " 'welches',\n",
              " 'wenn',\n",
              " 'werde',\n",
              " 'werden',\n",
              " 'wie',\n",
              " 'wieder',\n",
              " 'will',\n",
              " 'wir',\n",
              " 'wird',\n",
              " 'wirst',\n",
              " 'wo',\n",
              " 'wollen',\n",
              " 'wollte',\n",
              " 'wrde',\n",
              " 'wrden',\n",
              " 'zu',\n",
              " 'zum',\n",
              " 'zur',\n",
              " 'zwar',\n",
              " 'zwischen']"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Speech Of DR APJ Abdul Kalam\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for Indias development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isnt this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "lwdDRtwfe6IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "dMCD-vNfdikA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "wDcbx5r4eIIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "XpKHDWiqeIiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmt6pB2weIkD",
        "outputId": "43790c8e-1ac3-4117-e720-16ead04f017a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWSm0pqpeImc",
        "outputId": "4f8aea8f-0a8e-4740-c19b-67bc79831658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for Indias development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isnt this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7n0CqnqhUU7",
        "outputId": "e076b032-412d-4781-b176-10618baf93e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV5moQ3qhgcn",
        "outputId": "c8cf312b-fd4e-4af7-866c-117f8a96fda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'have', 'three', 'visions', 'for', 'India', '.']\n",
            "['In', '3000', 'years', 'of', 'our', 'history', ',', 'people', 'from', 'all', 'over', 'the', 'world', 'have', 'come', 'and', 'invaded', 'us', ',', 'captured', 'our', 'lands', ',', 'conquered', 'our', 'minds', '.']\n",
            "['From', 'Alexander', 'onwards', ',', 'the', 'Greeks', ',', 'the', 'Turks', ',', 'the', 'Moguls', ',', 'the', 'Portuguese', ',', 'the', 'British', ',', 'the', 'French', ',', 'the', 'Dutch', ',', 'all', 'of', 'them', 'came', 'and', 'looted', 'us', ',', 'took', 'over', 'what', 'was', 'ours', '.']\n",
            "['Yet', 'we', 'have', 'not', 'done', 'this', 'to', 'any', 'other', 'nation', '.']\n",
            "['We', 'have', 'not', 'conquered', 'anyone', '.']\n",
            "['We', 'have', 'not', 'grabbed', 'their', 'land', ',', 'their', 'culture', ',', 'their', 'history', 'and', 'tried', 'to', 'enforce', 'our', 'way', 'of', 'life', 'on', 'them', '.']\n",
            "['Why', '?']\n",
            "['Because', 'we', 'respect', 'the', 'freedom', 'of', 'others.That', 'is', 'why', 'my', 'first', 'vision', 'is', 'that', 'of', 'freedom', '.']\n",
            "['I', 'believe', 'that', 'India', 'got', 'its', 'first', 'vision', 'of', 'this', 'in', '1857', ',', 'when', 'we', 'started', 'the', 'War', 'of', 'Independence', '.']\n",
            "['It', 'is', 'this', 'freedom', 'that', 'we', 'must', 'protect', 'and', 'nurture', 'and', 'build', 'on', '.']\n",
            "['If', 'we', 'are', 'not', 'free', ',', 'no', 'one', 'will', 'respect', 'us', '.']\n",
            "['My', 'second', 'vision', 'for', 'India', '', 's', 'development', '.']\n",
            "['For', 'fifty', 'years', 'we', 'have', 'been', 'a', 'developing', 'nation', '.']\n",
            "['It', 'is', 'time', 'we', 'see', 'ourselves', 'as', 'a', 'developed', 'nation', '.']\n",
            "['We', 'are', 'among', 'the', 'top', '5', 'nations', 'of', 'the', 'world', 'in', 'terms', 'of', 'GDP', '.']\n",
            "['We', 'have', 'a', '10', 'percent', 'growth', 'rate', 'in', 'most', 'areas', '.']\n",
            "['Our', 'poverty', 'levels', 'are', 'falling', '.']\n",
            "['Our', 'achievements', 'are', 'being', 'globally', 'recognised', 'today', '.']\n",
            "['Yet', 'we', 'lack', 'the', 'self-confidence', 'to', 'see', 'ourselves', 'as', 'a', 'developed', 'nation', ',', 'self-reliant', 'and', 'self-assured', '.']\n",
            "['Isn', '', 't', 'this', 'incorrect', '?']\n",
            "['I', 'have', 'a', 'third', 'vision', '.']\n",
            "['India', 'must', 'stand', 'up', 'to', 'the', 'world', '.']\n",
            "['Because', 'I', 'believe', 'that', 'unless', 'India', 'stands', 'up', 'to', 'the', 'world', ',', 'no', 'one', 'will', 'respect', 'us', '.']\n",
            "['Only', 'strength', 'respects', 'strength', '.']\n",
            "['We', 'must', 'be', 'strong', 'not', 'only', 'as', 'a', 'military', 'power', 'but', 'also', 'as', 'an', 'economic', 'power', '.']\n",
            "['Both', 'must', 'go', 'hand-in-hand', '.']\n",
            "['My', 'good', 'fortune', 'was', 'to', 'have', 'worked', 'with', 'three', 'great', 'minds', '.']\n",
            "['Dr.', 'Vikram', 'Sarabhai', 'of', 'the', 'Dept', '.']\n",
            "['of', 'space', ',', 'Professor', 'Satish', 'Dhawan', ',', 'who', 'succeeded', 'him', 'and', 'Dr.', 'Brahm', 'Prakash', ',', 'father', 'of', 'nuclear', 'material', '.']\n",
            "['I', 'was', 'lucky', 'to', 'have', 'worked', 'with', 'all', 'three', 'of', 'them', 'closely', 'and', 'consider', 'this', 'the', 'great', 'opportunity', 'of', 'my', 'life', '.']\n",
            "['I', 'see', 'four', 'milestones', 'in', 'my', 'career']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "JJhqqL4MlHuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [ porter_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]= ' '.join(words) # converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "Cm1UqO6ahgfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0MW4StzhegB",
        "outputId": "16930070-c7e6-4a96-9f23-1bf61ec3e325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation .',\n",
              " 'we conquer anyon .',\n",
              " 'we grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'i believ india got first vision 1857 , start war independ .',\n",
              " 'it freedom must protect nurtur build .',\n",
              " 'if free , one respect us .',\n",
              " 'my second vision india  develop .',\n",
              " 'for fifti year develop nation .',\n",
              " 'it time see develop nation .',\n",
              " 'we among top 5 nation world term gdp .',\n",
              " 'we 10 percent growth rate area .',\n",
              " 'our poverti level fall .',\n",
              " 'our achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " 'isn  incorrect ?',\n",
              " 'i third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus i believ unless india stand world , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'we must strong militari power also econom power .',\n",
              " 'both must go hand-in-hand .',\n",
              " 'my good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'i lucki work three close consid great opportun life .',\n",
              " 'i see four mileston career']"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.sent_tokenize(sentences[i])\n",
        "  stem_words = [snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(stem_words)"
      ],
      "metadata": {
        "id": "rbuA5HMRm8gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGSN99dGsSzf",
        "outputId": "55f4f21c-0d84-4662-adee-97a5027066fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i have three visions for india.',\n",
              " 'in 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'from alexander onwards, the greeks, the turks, the moguls, the portuguese, the british,\\n               the french, the dutch, all of them came and looted us, took over what was ours.',\n",
              " 'yet we have not done this to any other nation.',\n",
              " 'we have not conquered anyone.',\n",
              " 'we have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
              " 'why?',\n",
              " 'because we respect the freedom of others.that is why my \\n               first vision is that of freedom.',\n",
              " 'i believe that india got its first vision of \\n               this in 1857, when we started the war of independence.',\n",
              " 'it is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'if we are not free, no one will respect us.',\n",
              " \"my second vision for india's development.\",\n",
              " 'for fifty years we have been a developing nation.',\n",
              " 'it is time we see ourselves as a developed nation.',\n",
              " 'we are among the top 5 nations of the world\\n               in terms of gdp.',\n",
              " 'we have a 10 percent growth rate in most areas.',\n",
              " 'our poverty levels are falling.',\n",
              " 'our achievements are being globally recognised today.',\n",
              " 'yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " \"isn't this incorrect?\",\n",
              " 'i have a third vision.',\n",
              " 'india must stand up to the world.',\n",
              " 'because i believe that unless india \\n               stands up to the world, no one will respect us.',\n",
              " 'only strength respects strength.',\n",
              " 'we must be \\n               strong not only as a military power but also as an economic power.',\n",
              " 'both must go hand-in-hand.',\n",
              " 'my good fortune was to have worked with three great minds.',\n",
              " 'dr. vikram sarabhai of the dept.',\n",
              " 'of \\n               space, professor satish dhawan, who succeeded him and dr. brahm prakash, father of nuclear material.',\n",
              " 'i was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'i see four milestones in my car']"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Speech Of DR APJ Abdul Kalam\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for Indias development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isnt this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "S7dRwCoJuYwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(paragraph)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejKzOVB0uYzE",
        "outputId": "4d17437b-fb04-4001-d7fe-03c7208a898a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for Indias development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isnt this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "60yT8EIJtrsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Lemmatizing\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  lemma_word = [wordnet_lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i] = ' '.join(lemma_word)"
      ],
      "metadata": {
        "id": "woaRfZ6ftru-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC1smwv0trxX",
        "outputId": "e3376ac7-01a5-4074-d48c-e8b0738b0f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three visions india .',\n",
              " '3000 years history , people world come invade us , capture land , conquer mind .',\n",
              " 'alexander onwards , greeks , turks , moguls , portuguese , british , french , dutch , come loot us , take .',\n",
              " 'yet nation .',\n",
              " 'conquer anyone .',\n",
              " 'grab land , culture , history try enforce way life .',\n",
              " '?',\n",
              " 'respect freedom others.that first vision freedom .',\n",
              " 'believe india get first vision 1857 , start war independence .',\n",
              " 'freedom must protect nurture build .',\n",
              " 'free , one respect us .',\n",
              " 'second vision india  development .',\n",
              " 'fifty years develop nation .',\n",
              " 'time see develop nation .',\n",
              " 'among top 5 nations world term gdp .',\n",
              " '10 percent growth rate areas .',\n",
              " 'poverty level fall .',\n",
              " 'achievements globally recognise today .',\n",
              " 'yet lack self-confidence see develop nation , self-reliant self-assured .',\n",
              " ' incorrect ?',\n",
              " 'third vision .',\n",
              " 'india must stand world .',\n",
              " 'believe unless india stand world , one respect us .',\n",
              " 'strength respect strength .',\n",
              " 'must strong military power also economic power .',\n",
              " 'must go hand-in-hand .',\n",
              " 'good fortune work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear material .',\n",
              " 'lucky work three closely consider great opportunity life .',\n",
              " 'see four milestones career']"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JP1bYOkvtrzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "#POS (Part-of-Speech)\n",
        "In NLP, **POS (Part-of-Speech) tagging** is the process of assigning a grammatical category (like noun, verb, adjective, adverb, pronoun, etc.) to each word in a given text.\n",
        "\n",
        "It's a foundational step because it helps algorithms understand the **grammatical structure and role of words within a sentence**, which is crucial for more advanced NLP tasks. For example, it helps disambiguate words that have multiple meanings based on their context (e.g., \"bank\" as a financial institution vs. \"bank\" as the side of a river)."
      ],
      "metadata": {
        "id": "qalIoJ2aP85B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS taggs\n",
        "'''\n",
        "CC coordinating conjunction\n",
        "CD cardinal digit\n",
        "DT determiner\n",
        "EX existential there (like: there is  think of it like there exists)\n",
        "FW foreign word\n",
        "IN preposition/subordinating conjunction\n",
        "JJ adjective  big\n",
        "JJR adjective, comparative  bigger\n",
        "JJS adjective, superlative  biggest\n",
        "LS list marker 1)\n",
        "MD modal  could, will\n",
        "NN noun, singular - desk\n",
        "NNS noun plural  desks\n",
        "NNP proper noun, singular  Harrison\n",
        "NNPS proper noun, plural  Americans\n",
        "PDT predeterminer  all the kids\n",
        "POS possessive ending parents\n",
        "PRP personal pronoun   I, he, she\n",
        "PRP$ possessive pronoun  my, his, hers\n",
        "RB adverb  very, silently,\n",
        "RBR adverb, comparative  better\n",
        "RBS adverb, superlative  best\n",
        "RP particle  give up\n",
        "TO  to go to the store.\n",
        "UH interjection  errrrrrrrm\n",
        "VB verb, base form  take\n",
        "VBD verb, past tense  took\n",
        "VBG verb, gerund/present participle  taking\n",
        "VBN verb, past participle  taken\n",
        "VBP verb, sing. present, non-3d  take\n",
        "VBZ verb, 3rd person sing. present  takes\n",
        "WDT wh-determiner  which\n",
        "WP wh-pronoun  who, what\n",
        "WP$ possessive wh-pronoun, eg- whose\n",
        "WRB wh-adverb, eg- where, when\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Y40zXRyFRLO-",
        "outputId": "7472322e-5297-4464-cfa3-4ab30b266fba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCC coordinating conjunction \\nCD cardinal digit \\nDT determiner \\nEX existential there (like: there is  think of it like there exists) \\nFW foreign word \\nIN preposition/subordinating conjunction \\nJJ adjective  big \\nJJR adjective, comparative  bigger \\nJJS adjective, superlative  biggest \\nLS list marker 1) \\nMD modal  could, will \\nNN noun, singular - desk \\nNNS noun plural  desks \\nNNP proper noun, singular  Harrison \\nNNPS proper noun, plural  Americans \\nPDT predeterminer  all the kids \\nPOS possessive ending parents \\nPRP personal pronoun   I, he, she \\nPRP$ possessive pronoun  my, his, hers \\nRB adverb  very, silently, \\nRBR adverb, comparative  better \\nRBS adverb, superlative  best \\nRP particle  give up \\nTO  to go to the store. \\nUH interjection  errrrrrrrm \\nVB verb, base form  take \\nVBD verb, past tense  took \\nVBG verb, gerund/present participle  taking \\nVBN verb, past participle  taken \\nVBP verb, sing. present, non-3d  take \\nVBZ verb, 3rd person sing. present  takes \\nWDT wh-determiner  which \\nWP wh-pronoun  who, what \\nWP$ possessive wh-pronoun, eg- whose \\nWRB wh-adverb, eg- where, when\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Speech Of DR APJ Abdul Kalam\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for Indias development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isnt this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "-7C7itu0RXnW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "bwvOUXdfScNr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rREw4p_OScP1",
        "outputId": "9b013066-fed5-4fb4-9624-23dac931f246"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over\\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture,\\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my\\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of\\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for Indias development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isnt this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India\\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be\\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of\\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will find out the POS Tag\n",
        "#nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.tag import pos_tag\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  word_tag = pos_tag(words)\n",
        "  print(word_tag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0113dq2iScUN",
        "outputId": "eede6f99-6708-4429-a84a-f4e3a844993f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('have', 'VBP'), ('three', 'CD'), ('visions', 'NNS'), ('for', 'IN'), ('India', 'NNP'), ('.', '.')]\n",
            "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('of', 'IN'), ('our', 'PRP$'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('from', 'IN'), ('all', 'DT'), ('over', 'IN'), ('the', 'DT'), ('world', 'NN'), ('have', 'VBP'), ('come', 'VBN'), ('and', 'CC'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('our', 'PRP$'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('our', 'PRP$'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('the', 'DT'), ('Greeks', 'NNP'), (',', ','), ('the', 'DT'), ('Turks', 'NNP'), (',', ','), ('the', 'DT'), ('Moguls', 'NNP'), (',', ','), ('the', 'DT'), ('Portuguese', 'NNP'), (',', ','), ('the', 'DT'), ('British', 'JJ'), (',', ','), ('the', 'DT'), ('French', 'JJ'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), (',', ','), ('all', 'DT'), ('of', 'IN'), ('them', 'PRP'), ('came', 'VBD'), ('and', 'CC'), ('looted', 'VBD'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('over', 'RP'), ('what', 'WP'), ('was', 'VBD'), ('ours', 'PRP'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('we', 'PRP'), ('have', 'VBP'), ('not', 'RB'), ('done', 'VBN'), ('this', 'DT'), ('to', 'TO'), ('any', 'DT'), ('other', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('have', 'VBP'), ('not', 'RB'), ('conquered', 'VBN'), ('anyone', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('have', 'VBP'), ('not', 'RB'), ('grabbed', 'VBN'), ('their', 'PRP$'), ('land', 'NN'), (',', ','), ('their', 'PRP$'), ('culture', 'NN'), (',', ','), ('their', 'PRP$'), ('history', 'NN'), ('and', 'CC'), ('tried', 'VBD'), ('to', 'TO'), ('enforce', 'VB'), ('our', 'PRP$'), ('way', 'NN'), ('of', 'IN'), ('life', 'NN'), ('on', 'IN'), ('them', 'PRP'), ('.', '.')]\n",
            "[('Why', 'WRB'), ('?', '.')]\n",
            "[('Because', 'IN'), ('we', 'PRP'), ('respect', 'VBP'), ('the', 'DT'), ('freedom', 'NN'), ('of', 'IN'), ('others.That', 'WP'), ('is', 'VBZ'), ('why', 'WRB'), ('my', 'PRP$'), ('first', 'JJ'), ('vision', 'NN'), ('is', 'VBZ'), ('that', 'IN'), ('of', 'IN'), ('freedom', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('believe', 'VBP'), ('that', 'IN'), ('India', 'NNP'), ('got', 'VBD'), ('its', 'PRP$'), ('first', 'JJ'), ('vision', 'NN'), ('of', 'IN'), ('this', 'DT'), ('in', 'IN'), ('1857', 'CD'), (',', ','), ('when', 'WRB'), ('we', 'PRP'), ('started', 'VBD'), ('the', 'DT'), ('War', 'NNP'), ('of', 'IN'), ('Independence', 'NNP'), ('.', '.')]\n",
            "[('It', 'PRP'), ('is', 'VBZ'), ('this', 'DT'), ('freedom', 'NN'), ('that', 'IN'), ('we', 'PRP'), ('must', 'MD'), ('protect', 'VB'), ('and', 'CC'), ('nurture', 'NN'), ('and', 'CC'), ('build', 'VB'), ('on', 'IN'), ('.', '.')]\n",
            "[('If', 'IN'), ('we', 'PRP'), ('are', 'VBP'), ('not', 'RB'), ('free', 'JJ'), (',', ','), ('no', 'DT'), ('one', 'NN'), ('will', 'MD'), ('respect', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('for', 'IN'), ('India', 'NNP'), ('', 'NNP'), ('s', 'NN'), ('development', 'NN'), ('.', '.')]\n",
            "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('we', 'PRP'), ('have', 'VBP'), ('been', 'VBN'), ('a', 'DT'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('is', 'VBZ'), ('time', 'NN'), ('we', 'PRP'), ('see', 'VBP'), ('ourselves', 'PRP'), ('as', 'IN'), ('a', 'DT'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('are', 'VBP'), ('among', 'IN'), ('the', 'DT'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('in', 'IN'), ('terms', 'NNS'), ('of', 'IN'), ('GDP', 'NNP'), ('.', '.')]\n",
            "[('We', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('10', 'CD'), ('percent', 'NN'), ('growth', 'NN'), ('rate', 'NN'), ('in', 'IN'), ('most', 'JJS'), ('areas', 'NNS'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('are', 'VBP'), ('falling', 'VBG'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('achievements', 'NNS'), ('are', 'VBP'), ('being', 'VBG'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
            "[('Yet', 'CC'), ('we', 'PRP'), ('lack', 'VBP'), ('the', 'DT'), ('self-confidence', 'NN'), ('to', 'TO'), ('see', 'VB'), ('ourselves', 'PRP'), ('as', 'IN'), ('a', 'DT'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('and', 'CC'), ('self-assured', 'JJ'), ('.', '.')]\n",
            "[('Isn', 'NNP'), ('', 'NNP'), ('t', 'NN'), ('this', 'DT'), ('incorrect', 'NN'), ('?', '.')]\n",
            "[('I', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
            "[('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('up', 'RP'), ('to', 'TO'), ('the', 'DT'), ('world', 'NN'), ('.', '.')]\n",
            "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), ('that', 'IN'), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('up', 'RP'), ('to', 'TO'), ('the', 'DT'), ('world', 'NN'), (',', ','), ('no', 'DT'), ('one', 'NN'), ('will', 'MD'), ('respect', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
            "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('must', 'MD'), ('be', 'VB'), ('strong', 'JJ'), ('not', 'RB'), ('only', 'RB'), ('as', 'IN'), ('a', 'DT'), ('military', 'JJ'), ('power', 'NN'), ('but', 'CC'), ('also', 'RB'), ('as', 'IN'), ('an', 'DT'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
            "[('Both', 'DT'), ('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('have', 'VB'), ('worked', 'VBN'), ('with', 'IN'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('Dr.', 'NNP'), ('Vikram', 'NNP'), ('Sarabhai', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Dept', 'NNP'), ('.', '.')]\n",
            "[('of', 'IN'), ('space', 'NN'), (',', ','), ('Professor', 'NNP'), ('Satish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('who', 'WP'), ('succeeded', 'VBD'), ('him', 'PRP'), ('and', 'CC'), ('Dr.', 'NNP'), ('Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'NN'), ('of', 'IN'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('was', 'VBD'), ('lucky', 'JJ'), ('to', 'TO'), ('have', 'VB'), ('worked', 'VBN'), ('with', 'IN'), ('all', 'DT'), ('three', 'CD'), ('of', 'IN'), ('them', 'PRP'), ('closely', 'RB'), ('and', 'CC'), ('consider', 'VB'), ('this', 'DT'), ('the', 'DT'), ('great', 'JJ'), ('opportunity', 'NN'), ('of', 'IN'), ('my', 'PRP$'), ('life', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('see', 'VBP'), ('four', 'CD'), ('milestones', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('career', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'Taj Mahal is a beautiful monument'"
      ],
      "metadata": {
        "id": "Rlt1cGiLScWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tag = pos_tag('Taj Mahal is a beautiful monument'.split())"
      ],
      "metadata": {
        "id": "fm6q94TaSci2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPPXiv9hUqvP",
        "outputId": "c72efb28-6bef-4f6a-b265-25d9378a83b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Taj', 'NNP'),\n",
              " ('Mahal', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('beautiful', 'JJ'),\n",
              " ('monument', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uK_6Uc4yU0jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "#Named Entity Recognition (NER)\n",
        "**Named Entity Recognition (NER)** is a subfield of Natural Language Processing (NLP) that aims to **identify and classify \"named entities\"** in unstructured text into predefined categories.\n",
        "\n",
        "These categories typically include:\n",
        "\n",
        "* **Person names** (e.g., \"Elon Musk\", \"Dr. Jane Smith\")\n",
        "* **Organizations** (e.g., \"Google\", \"United Nations\", \"NASA\")\n",
        "* **Locations** (e.g., \"Paris\", \"Mount Everest\", \"India\")\n",
        "* **Dates and Times** (e.g., \"July 4th, 1776\", \"next Tuesday\", \"3 PM\")\n",
        "* **Quantities/Monetary Values** (e.g., \"$500\", \"10,000 miles\", \"85%\")\n",
        "* And sometimes more domain-specific entities like \"medical codes,\" \"product names,\" or \"disease names.\"\n",
        "\n",
        "**In essence, NER helps computers understand *who*, *what*, *where*, and *when* specific pieces of information are mentioned in a text, transforming unstructured text into structured data.**\n",
        "\n",
        "For example, in the sentence: \"Apple Inc. is planning to open a new office in San Francisco in March 2025.\"\n",
        "\n",
        "An NER system would identify:\n",
        "\n",
        "* \"Apple Inc.\" as an **ORGANIZATION**\n",
        "* \"San Francisco\" as a **LOCATION**\n",
        "* \"March 2025\" as a **DATE**\n",
        "\n",
        "NER is a fundamental step for many advanced NLP applications, including information extraction, question answering, search engines, and building knowledge graphs."
      ],
      "metadata": {
        "id": "TUyBbsKOWbKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.chunk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dhi41feXYXU",
        "outputId": "e8798d54-e80a-4b6a-bacb-fd84c3e09abe"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
      ],
      "metadata": {
        "id": "dc1L0W0UXlvD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(sentence)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLPhcrp3Xlxf",
        "outputId": "41b2acca-a6cc-4543-97c6-95a9f32b0d85"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Eiffel',\n",
              " 'Tower',\n",
              " 'was',\n",
              " 'built',\n",
              " 'from',\n",
              " '1887',\n",
              " 'to',\n",
              " '1889',\n",
              " 'by',\n",
              " 'Gustave',\n",
              " 'Eiffel',\n",
              " ',',\n",
              " 'whose',\n",
              " 'company',\n",
              " 'specialized',\n",
              " 'in',\n",
              " 'building',\n",
              " 'metal',\n",
              " 'frameworks',\n",
              " 'and',\n",
              " 'structures',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag_words = nltk.pos_tag(words)\n",
        "pos_tag_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMqofgeoW5v1",
        "outputId": "75c3fea5-7422-46ee-d9b7-afe425e032bc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('Eiffel', 'NNP'),\n",
              " ('Tower', 'NNP'),\n",
              " ('was', 'VBD'),\n",
              " ('built', 'VBN'),\n",
              " ('from', 'IN'),\n",
              " ('1887', 'CD'),\n",
              " ('to', 'TO'),\n",
              " ('1889', 'CD'),\n",
              " ('by', 'IN'),\n",
              " ('Gustave', 'NNP'),\n",
              " ('Eiffel', 'NNP'),\n",
              " (',', ','),\n",
              " ('whose', 'WP$'),\n",
              " ('company', 'NN'),\n",
              " ('specialized', 'VBD'),\n",
              " ('in', 'IN'),\n",
              " ('building', 'NN'),\n",
              " ('metal', 'NN'),\n",
              " ('frameworks', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('structures', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "id": "qdCUQoZtY47k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ne_chunk(pos_tag_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drxIEYR4W5yU",
        "outputId": "1a5dfb99-2e25-47f5-dc5a-ef6fbae99270"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (ORGANIZATION Eiffel/NNP Tower/NNP)\n",
            "  was/VBD\n",
            "  built/VBN\n",
            "  from/IN\n",
            "  1887/CD\n",
            "  to/TO\n",
            "  1889/CD\n",
            "  by/IN\n",
            "  (PERSON Gustave/NNP Eiffel/NNP)\n",
            "  ,/,\n",
            "  whose/WP$\n",
            "  company/NN\n",
            "  specialized/VBD\n",
            "  in/IN\n",
            "  building/NN\n",
            "  metal/NN\n",
            "  frameworks/NNS\n",
            "  and/CC\n",
            "  structures/NNS\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ne_chunk(pos_tag_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "m6XECt0HW50G",
        "outputId": "b4bf759a-09e7-42e4-d3db-d5bddd8f4fa4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Eiffel', 'NNP'), ('Tower', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('from', 'IN'), ('1887', 'CD'), ('to', 'TO'), ('1889', 'CD'), ('by', 'IN'), Tree('PERSON', [('Gustave', 'NNP'), ('Eiffel', 'NNP')]), (',', ','), ('whose', 'WP$'), ('company', 'NN'), ('specialized', 'VBD'), ('in', 'IN'), ('building', 'NN'), ('metal', 'NN'), ('frameworks', 'NNS'), ('and', 'CC'), ('structures', 'NNS'), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1280.0,168.0\" width=\"1280px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"3.125%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.5625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.375%\" x=\"3.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tower</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.125%\" x=\"12.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.0625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"15.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">built</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"20%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.875%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"23.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1887</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"27.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.75%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"30%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1889</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.875%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"33.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.625%\" x=\"36.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Gustave</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"20px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.5625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.875%\" x=\"46.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"48.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">whose</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50.9375%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.625%\" x=\"53.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.9375%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.125%\" x=\"58.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">specialized</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"66.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.25%\" x=\"69.375%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">building</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"75.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">metal</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"80%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">frameworks</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.75%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.125%\" x=\"87.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.0625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"90.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">structures</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.375%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.875%\" x=\"98.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.0625%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hbUcInLlZYBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}